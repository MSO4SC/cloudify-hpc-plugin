= Cloudify HPC Plugin
// Settings
:idprefix:
:idseparator: -
//ifndef::env-github[:icons: font]
ifdef::env-github,env-browser[]
:toc: macro
:toclevels: 1
endif::[]
ifdef::env-github[]
:branch: master
:status:
:outfilesuffix: .adoc
:!toc-title:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:icons:
// URIs
:uri-vagrant: https://www.vagrantup.com/
:uri-docker: https://www.docker.com/
:uri-ci-travis: https://travis-ci.org/MSO4SC/cloudify-hpc-plugin
:uri-cloudify: http://cloudify.co/
:uri-cloudify-docs: http://docs.getcloudify.org/4.1.0/intro/what-is-cloudify/
:uri-cloudify-use-plugin: http://docs.getcloudify.org/4.1.0/plugins/using-plugins/
:uri-cloudify-types: http://docs.getcloudify.org/4.1.0/blueprints/spec-node-types/
:uri-cloudify-builtin-types: http://docs.getcloudify.org/4.1.0/blueprints/built-in-types/
:uri-cloudify-relationships: http://docs.getcloudify.org/4.1.0/blueprints/spec-relationships/
:uri-mso4sc: http://www.mso4sc.eu/
:uri-mso4sc-dockerhub: https://hub.docker.com/u/mso4sc/dashboard/
:uri-blueprint-examples: https://github.com/MSO4SC/resources/tree/master/blueprint-examples
:uri-monitor-orchestrator: https://github.com/MSO4SC/exporter_orchestrator
:uri-msoorchestrator-cli: https://github.com/MSO4SC/msoorchestrator-cli
:uri-slurm-exporter: https://github.com/MSO4SC/slurm_exporter
:uri-prometheus: https://prometheus.io/
:uri-singularity: http://singularity.lbl.gov/
:uri-slurm: https://slurm.schedmd.com/
:uri-torque: http://www.adaptivecomputing.com/products/open-source/torque/
:uri-grafana: https://grafana.com/
:uri-grafana-mso4sc-dashboard: https://github.com/MSO4SC/MSOMonitor/blob/master/grafana/MSO4SC.json
:uri-monitor-compose: https://github.com/MSO4SC/MSOMonitor/blob/master/docker-compose.yml
:uri-original-repo: https://github.com/MSO4SC/cloudify-hpc-plugin

NOTE: Originally developed under the H2020 project {uri-mso4sc}[MSO4SC]: {uri-original-repo}

A HPC plugin for {uri-cloudify}[Cloudify] that enables it to manage HPC resources in one or more infrastructures. The currently supported resource types are described below.

This plugin is part of the {uri-mso4sc}[MSO4SC H2020 European Project].

TIP: Example blueprints can be found at the {uri-blueprint-examples}[MSO4SC resources repository].

toc::[]


== Overview

The plugin aims to enable {uri-cloudify}[Cloudify] to manage HPC resources, so at the end, combined with other plugins, it can orchestrate a hybrid cloud+hpc environment, with one or more cloud and hpc providerds at the same time.

It adds a new resource type, <<hpc.nodes.WorkloadManager>>, that represents an infrastructure interface (Slurm, Torque, or even Bash), and <<hpc.nodes.Job>> and <<hpc.nodes.SingularityJob>>, that represents a job in the HPC and a job using a {uri-singularity}[Singularity] container respectively.

In order to Cloudify orchestrates properly the HPC resources, the help of an external monitor system is needed, from which the job status is retrieved. In the next release the plugin will also use the monitor to predict the overall state of the HPC and take best decisions about which partition and infrastructure to use for each job.

IMPORTANT: Only {uri-slurm}[Slurm] and {uri-torque}[Torque] based HPCs are supported for now (Bash for virtual machines), as well as only {uri-prometheus}[Prometheus] is supported as external monitoring system.



== Plugin Requirements

* Python version 2.7.x
* Access to at least a {uri-slurm}[Slurm] based HPC by ssh user & password.

=== Optional:

* Access to {uri-torque}[Moab/Torque] based HPC by ssh user & password.
* The {uri-monitor-orchestrator}[Monitor Orchestrator] can be deployed in the same host as the monitor to allow the plugin to dynamically use new HPC infrastructures defined in TOSCA.
* {uri-grafana}[Grafana] can be used to visualize the status of the HPCs.
* {uri-prometheus}[Prometheus] monitoring the infrastructures to be used ({uri-slurm-exporter}[Slurm exporter] has been developed for this purpose)


== Installation & Usage

The plugin is installed as any other plugin. Check {uri-cloudify-docs}[Cloudify Docs] for general information about how to install and use Cloudify, and {uri-cloudify-use-plugin}[this section] for concrete information about using plugins.

Additionally {uri-mso4sc}[MSO4SC] provide {uri-vagrant}[Vagrant] and {uri-docker}[Docker] images at {uri-mso4sc-dockerhub}[Docker Hub] to install everything. *Check {uri-msoorchestrator-cli}[MSOOrchestrator-CLI] to start using Cloudify CLI and bootstrap the Cloudify Manager. Use {uri-monitor-compose}[docker compose file] to deploy all the external components. A {uri-grafana}[Grafana] dashboard can be found {uri-grafana-mso4sc-dashboard}[here].*

== HPC Plugin Configuration

The HPC plugin requires credentials, endpoint and other setup information in order to authenticate and interact with them.

This configuration properties are defined in <<hpc.nodes.Compute>> _credentials_ and _config_ properties.

[source,yaml]
----
credentials:
    host: "[HPC-HOST]"
    user: "[HPC-SSH-USER]"
    private_key: |
        -----BEGIN RSA PRIVATE KEY-----
        ......
        -----END RSA PRIVATE KEY-----
    private_key_password: "[PRIVATE-KEY-PASSWORD]"
    password: "[HPC-SSH-PASS]"
    login_shell: {true|false}
    tunnel:
        host: ...
        ...
----

. HPC and ssh credentials. At least `private_key` or `password` must be provided.
.. _tunnel_: Follows the same structure as its parent (credentials), to connect to the HPC through an tunneled SSH connection.

[source,yaml]
----
config:
    country_tz: "Europe/Madrid"
    workload_manager: {"SLURM"|"TORQUE"}
----

. _country_tz_: Country Time Zone configured in the the HPC.
. _workload_manager_: Workload manager used by the HPC.

WARNING: Only Slurm and Torque are currently accepted as workload managers.


== Types

This section describes the {uri-cloudify-types}[node type] definitions. Nodes describe resources in your HPC infrastructures. For more information, see {uri-cloudify-types}[node type].

=== hpc.nodes.WorkloadManager

**Derived From:** {uri-cloudify-builtin-types}[cloudify.nodes.Compute]

Use this type to describe a HPC infrastructure.

**Properties:**

* `credentials`: Connection credentials, as described in <<hpc-config-properties>>.
* `config`: HPC configuration, as described in <<hpc-config-properties>>.
* `external_monitor_entrypoint`: Entrypoint of the external monitor that Cloudify will use instead of the internal one.
* `external_monitor_port`: Port of the monitor. Default `:9090`.
* `external_monitor_type`: Specific monitor tool. Default `{uri-prometheus}[PROMETHEUS]`.
* `external_monitor_orchestrator_port`: Monitor orchestrator port. Default `:8079`.
* `job_prefix`: Job name prefix for the jobs created in this HPC. Default `cfyhpc`.
* `base_dir`: Root directory in which to run the executions in this ifrastructure. Default `$HOME`.
* `workdir_prefix`: Prefix name of the working directory that will be created for this infrastructure.
* `monitor_period`: Seconds to check job status. This is necessary because workload managers can be overloaded if asked too much times in a short period of time. Default `60`.
* `skip_cleanup`: True to not clean all files when destroying the deployment. Default `False`.
* `simulate`: If true, don't send the jobs to the HPC and simulate that they finish inmediately. Useful for test new TOSCA files. Default `False`.

*Example*

This example demonstrates how to add a new HPC.

[source,yaml]
----

    hpc_wm:
        type: hpc.nodes.WorkloadManager
        properties:
            credentials:
                host: "[HPC-HOST]"
                user: "[HPC-SSH-USER]"
                password: "[HPC-SSH-PASS]"
                login_shell: false
            config:
                country_tz: "Europe/Madrid"
                workload_manager: "SLURM"
            job_prefix: wm_
            workdir_prefix: test
...

----

*Mapped Operations:*

* `cloudify.interfaces.lifecycle.configure` Checks that there is connection between Cloudify and the HPC, and creates a new working directory.
* `cloudify.interfaces.lifecycle.delete` Clean up all data generated by the execution.
* `cloudify.interfaces.monitoring.start` If the external monitor orchestrator is available, sends a notification to start monitoring the HPC.
* `cloudify.interfaces.monitoring.stop` If the external monitor orchestrator is available, sends a notification to end monitoring the HPC.

=== hpc.nodes.Job

**Derived From:** {uri-cloudify-builtin-type}[cloudify.nodes.Root]

Use this tipe to describe a HPC job.

**Properties:**

* `job_options`: Job parameters and needed resources.
** `type`: SRUN or SBATCH (job executed using a command or using a script). TORQUE supports only SBATCH mode.
** `pre`: List of commands to be executed before running the job. Optional.
** `post`: List of commands to be executed after running the job. Optional.
** `partition`: Partition in which the job will be executed. If not provided, the HPC default will be used.
** `command`: Job executable command with arguments if necessary. Since TORQUE does NOT accept extra arguments in job submission command `qsub`, this field must contain only a name of the batch script to run for TORQUE. Mandatory.
** `nodes`: Necessary nodes of the job. Default `1`.
** `tasks`: Number of tasks of the job. Default `1`.
** `tasks_per_node`: Number of tasks per node. Default `1`.
** `max_time`: Set a limit on the total run time of the job allocation. Mandatory if SRUN type.
** `scale`: Execute in parallel the job N times according to this property. Only works with SBATCH jobs. Default `1` (no scale).
** `scale_max_in_parallel`: Maximum number of scaled job instances that can be run in parallel. Only works with scale > `1`. Default same as scale.
** `memory`: Specify the real memory required per node.  Different units can be specified using the suffix [`K|M|G|T`]. Default value `""` lets the workload manager assign the default memory to the job.
** `stdout_file`: Define the file where to gather the standard output of the job. Default value `""` sets `<job-name>.err` filename.
** `stderr_file`: Define the file where to gather the standard error output. Default value `""` sets `<job-name>.out` filename.
** `mail-user`: Email to receive notification of job state changes. Default value `""` does not send any mail.
** `mail-type`: Type of event to be notified by mail, can define several events separated by comma. Valid values `NONE, BEGIN, END, FAIL, TIME_LIMIT, REQUEUE, ALL`. Default value `""` does not send any mail.
** `reservation`: Allocate resources for the job from the named reservation. Default value `""` does not allocate from any named reservation.
** `qos`: Request a quality of service for the job. Default value `""` lets de workload manager assign the default user `qos`.
* `deployment`: Scripts to perform deployment operations. Optional.
** `bootstrap`: Relative path to blueprint to the script that will be executed in the HPC at the install workflow to bootstrap the job (like data movements, binary download, etc.)
** `revert`: Relative path to blueprint to the script that will be executed in the HPC at the uninstall workflow, reverting the bootstrap or other clean up operations.
** `inputs`: List of inputs that will be passed to the scripts when executed in the HPC.
* `publish`: A list of outputs to be published after job execution. Each list item is a dictionary containing:
** `type`: Type of the external repository to be published. Only `CKAN` is supported for now. The rest of the parameters depends on the type.
** `type: CKAN`
*** `entrypoint`: ckan entrypoint
*** `api_key`: Individual user ckan api key.
*** `dataset`: Id of the dataset in which the file will be published.
*** `file_path`: Local path of the output file in the computation node.
*** `name`: Name used to publish the file in the repository.
*** `description`: Text describing the data file.
* `skip_cleanup`: Set to true to not clean up orchestrator auxiliar files. Default `False`.

NOTE: The variable $CURRENT_WORKDIR is available in all operations and scripts. It points to the working directory of the execution in the HPC from the _HOME_ directory: `/home/user/$CURRENT_WORKDIR/`.

NOTE: The variables `$SCALE_INDEX`, `$SCALE_COUNT` and `$SCALE_MAX` will be available in the batch script if the line `# DYNAMIC VARIABLES` exist (they will be dynamicaly loaded after this line). They hold, for each job instance, the index, the total number of instances, and the maximun in parallel respectively.

*Example*

This example demonstrates how to describe a new job for non-batched run (in Slurm).

[source,yaml]
----
    hpc_job:
        type: hpc.nodes.Job
        properties:
            job_options:
                type: 'SRUN'
                pre:
                    - module load gcc/5.3.0
                partition: 'thin-shared'
                command: 'touch example.test'
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:01:00'
            deployment:
                bootstrap: 'scripts/bootstrap_example.sh'
                revert: 'scripts/revert_example.sh'
                inputs:
                    - 'example_job'
...

----

This example demonstrates how to describe a new batch job (works with both Slurm and Torque).

[source,yaml]
----
    hpc_batch_job:
        type: hpc.nodes.job
        properties:
            job_options:
                type: 'SBATCH'
                command: "touch.script"
            deployment:
                bootstrap: 'scripts/bootstrap_sbatch_example.sh'
                revert: 'scripts/revert_sbatch_example.sh'
                inputs:
                    - 'single'
            skip_cleanup: True
        relationships:
            - type: job_contained_in_hpc
              target: first_hpc
...

----

*Mapped Operations:*

* `cloudify.interfaces.lifecycle.start` Send and execute the bootstrap script.
* `cloudify.interfaces.lifecycle.stop` Send and execute the revert script.
* `hpc.interfaces.lifecycle.queue` Queues the job in the HPC.
* `hpc.interfaces.lifecycle.publish` Publish outputs outside the HPC.
* `hpc.interfaces.lifecycle.cleanup` Clean up operations after job is finished.
* `hpc.interfaces.lifecycle.cancel` Cancels a queued job.



=== hpc.nodes.SingularityJob

**Derived From:** <<hpc.nodes.job>>

Use this tipe to describe a HPC job executed from a {uri-singularity}[Singularity] image.
Note that in this version TORQUE does not support Singularity jobs yet.

**Properties:**

* `job_options`: Job parameters and needed resources.
** `pre`: List of commands to be executed before running singularity container. Optional.
** `post`: List of commands to be executed after running singularity container. Optional.
** `image`: {uri-singularity}[Singularity] image file.
** `home`: Home volume that will be bind with the image instance (Optional).
** `volumes`: List of volumes that will be bind with the image instance.
** `partition`: Partition in which the job will be executed. If not provided, the HPC default will be used.
** `nodes`: Necessary nodes of the job. 1 by default.
** `tasks`: Number of tasks of the job. 1 by default.
** `tasks_per_node`: Number of tasks per node. 1 by default.
** `max_time`: Set a limit on the total run time of the job allocation. Mandatory if SRUN type.
** `scale`: Execute in parallel the job N times according to this property. Default `1` (no scale).
** `scale_max_in_parallel`: Maximum number of scaled job instances that can be run in parallel. Only works with scale > `1`. Default same as scale.
** `memory`: Specify the real memory required per node.  Different units can be specified using the suffix [`K|M|G|T`]. Default value `""` lets the workload manager assign the default memory to the job.
** `stdout_file`: Define the file where to gather the standard output of the job. Default value `""` sets `<job-name>.err` filename.
** `stderr_file`: Define the file where to gather the standard error output. Default value `""` sets `<job-name>.out` filename.
** `mail-user`: Email to receive notification of job state changes. Default value `""` does not send any mail.
** `mail-type`: Type of event to be notified by mail, can define several events separated by comma. Valid values `NONE, BEGIN, END, FAIL, TIME_LIMIT, REQUEUE, ALL`. Default value `""` does not send any mail.
** `reservation`: Allocate resources for the job from the named reservation. Default value `""` does not allocate from any named reservation.
** `qos`: Request a quality of service for the job. Default value `""` lets de workload manager assign the default user `qos`.
* `deployment`: Optional scripts to perform deployment operations (bootstrap and revert).
** `bootstrap`: Relative path to blueprint to the script that will be executed in the HPC at the install workflow to bootstrap the job (like image download, data movements, etc.)
** `revert`: Relative path to blueprint to the script that will be executed in the HPC at the uninstall workflow, reverting the bootstrap or other clean up operations (like removing the image).
** `inputs`: List of inputs that will be passed to the scripts when executed in the HPC
* `skip_cleanup`: Set to true to not clean up orchestrator auxiliar files. Default `False`.

NOTE: The variable $CURRENT_WORKDIR is available in all operations and scripts. It points to the working directory of the execution in the HPC from the _HOME_ directory: `/home/user/$CURRENT_WORKDIR/`.

NOTE: The variables $SCALE_INDEX, $SCALE_COUNT and $SCALE_MAX are available when scaling, holding for each job instance the index, the total number of instances, and the maximun in parallel respectively.

*Example*

This example demonstrates how to describe a new job executed in a {uri-singularity}[Singularity] instance.

[source,yaml]
----
    singularity_job:
        type: hpc.nodes.SingularityJob
        properties:
            job_options:
                pre:
                    - module load gcc/5.3.0 openmpi/1.10.2
                    - module load singularity/2.3.1
                    - touch pre.output
                partition: 'thin-shared' 
                post:
                    - touch post.output
                image: '$LUSTRE/openmpi_1.10.7_ring.img'
                home: '$HOME:/home/$USER'
                volumes:
                    - '/scratch'
                command: 'ring > fourth_example_3.test'
                nodes: 1
                tasks: 1
                tasks_per_node: 1
                max_time: '00:01:00'
            deployment:
                bootstrap: 'scripts/singularity_bootstrap_example.sh'
                revert: 'scripts/singularity_revert_example.sh'
                inputs:
                    - 'singularity_job'
...

----

*Mapped Operations:*

* `cloudify.interfaces.lifecycle.start` Send and execute the bootstrap script.
* `cloudify.interfaces.lifecycle.stop` Send and execute the revert script.
* `hpc.interfaces.lifecycle.queue` Queues the job in the HPC.
* `hpc.interfaces.lifecycle.publish` Publish outputs outside the HPC.
* `hpc.interfaces.lifecycle.cleanup` Clean up operations after job is finished.
* `hpc.interfaces.lifecycle.cancel` Cancels a queued job.



== Relationships

See the {uri-cloudify-relationships}[relationships] section.

The following plugin relationship operations are defined in the HPC plugin:

* `job_managed_by_wm` Sets a <<hpc.nodes.Job>> to be executed inside the target <<hpc.nodes.WorkloadManager>>.

* `job_depends_on` Sets a <<hpc.nodes.Job>> as a dependency of the target (another <<hpc.nodes.Job>>), so the target job needs to finish before the source can start.

* `wm_contained_in` Sets a <<hpc.nodes.WorkloadManager>> to be contained in the specific target (a computing node).


== Tests

To run the tests Cloudify CLI has to be installed locally. Example blueprints can be found at _tests/blueprint_ folder and have the `simulate` option active by default. Blueprint to be tested can be changed at _workflows_tests.py_ in the _tests_ folder.

To run the tests against a real HPC / Monitor system, copy the file _blueprint-inputs.yaml_ to _local-blueprint-inputs.yaml_ and edit with your credentials. Then edit the blueprint commenting the simulate option, and other parameters as you wish (e.g change the name ft2_node for your own hpc name). To use the openstack integration, your private key must be put in the folder _inputs/keys_.

[NOTE]
==========================
_dev-requirements.txt_ needs to be installed (_windev-requirements.txt_ for windows):
[source,bash]
----
pip install -r dev-requirements.txt
----

To run the tests, run tox on the root folder
[source,bash]
----
tox -e flake8,py27
----
==========================
